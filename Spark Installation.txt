1 Python 3.10 (cmds check: python --version) (check python paths: where python) (check pyspark paths: where pyspark)
  Always run python -m pip install … instead of just pip install … to make sure it uses the Python version you want.
  python -m pip install numpy pandas matplotlib seaborn scikit-learn jupyterlab xgboost pyspark==3.5.1

2 Java jdk 17 or 21 (cmd check: java --version)
  Download the excutable and follow default steps.

3 Hadoop (install corresponding hadoop version for this case "Python 3.10.11 && yspark==3.5.1" I used hadoop version 3.3.6)
  Download all files and put them in this directory (after creating it): C:\hadoop\bin

4 Environment variables:
  Windwos Search > View advanced system settings > Environment Variables... > System variables:
  - New:
    * Variable name: HADOOP_HOME
      Variable value: C:\hadoop

    * Variable name: JAVA_HOME
      Variable value: C:\Program Files\Java\jdk-17

  - Path > Edit... > New:
    * C:\hadoop\bin
    * %JAVA_HOME%\bin

  - Do some checks:
    * echo %HADOOP_HOME%
    * echo %JAVA_HOME%

this is enough for PySpark to work on VSCode (do not forget Ctrl Shift P for the interpreter Python 3.10 and
condsider using environment for later projects).

cmd or inside VSCode:
Generating a txt file for dependencies/versions: python -m pip freeze > requirements.txt (install every package to the env
otherwise you'll get an empty file)
python -m pip install -r requirements.txt (not for me put it as a github command for others to test your work
with the same requirements)
- Create a project folder for an environment manually and place it in user for reuse (cmd access that folder cd path)
- Create an environment inside the folder: python -m venv env
- Activate it: env\scripts\activate
- install pyspark: python -m pip install pyspark==3.5.1
- Generating a txt file for dependencies/versions: python -m pip freeze > requirements.txt
---------------------------------------------------------------------------------------------------------------------------
PySpark on Jupyter lab:
- Create a project folder for an environment manually and place it in user for reuse (cmd access that folder cd path)
- Create an environment inside the folder: python -m venv env
- Activate it: env\scripts\activate
- install pyspark: python -m pip install pyspark==3.5.1
- then: python -m pip install findspark
- Jupyter lab: python -m pip install jupyterlab
- Launch jupyter lab normally or using: jupyter-lab